{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMwRORcYagbh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import all modules\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Activation\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "#from keras_sequential_ascii import sequential_model_to_ascii_printout\n",
    "from keras import backend as K\n",
    "if K.backend()=='tensorflow':\n",
    "    K.set_image_dim_ordering(\"th\")\n",
    " \n",
    "# Import Tensorflow with multiprocessing\n",
    "import tensorflow as tf\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4xzcV3KZajZU"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "sIFVpaXdaqhx",
    "outputId": "6c62f5d4-7db3-4854-be6d-1eab1d2db42c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 3, 32, 32)\n",
      "y_train shape: (50000, 1)\n",
      "X_test shape: (10000, 3, 32, 32)\n",
      "y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xm2Vxr8da52q"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KvmAbEETD1ln"
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 32 \n",
    "# 32 examples in a mini-batch, smaller batch size means more updates in one epoch\n",
    " \n",
    "num_classes = 10 #\n",
    "epochs = 100 # repeat 100 times\n",
    "y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train  /= 255\n",
    "X_test /= 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ob7GKnH_g0T8"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4437
    },
    "colab_type": "code",
    "id": "7H9hP6L1awgz",
    "outputId": "d691d042-df2b-478c-8de2-ee6f2f7f9769"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_37 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 32, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 32, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 64, 16, 16)        18496     \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 64, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 64, 16, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 64, 16, 16)        36928     \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 64, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 64, 16, 16)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 64, 8, 8)          0         \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 64, 8, 8)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 128, 8, 8)         73856     \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 128, 8, 8)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 128, 8, 8)         32        \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 128, 8, 8)         147584    \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 128, 8, 8)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 128, 8, 8)         32        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 128, 4, 4)         0         \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 128, 4, 4)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 307,946\n",
      "Trainable params: 307,722\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n",
      "Epoch 1/125\n",
      "781/781 [==============================] - 48s 62ms/step - loss: 1.7384 - acc: 0.4262 - val_loss: 1.2228 - val_acc: 0.5831\n",
      "Epoch 2/125\n",
      "781/781 [==============================] - 43s 55ms/step - loss: 1.2235 - acc: 0.5854 - val_loss: 0.9822 - val_acc: 0.6774\n",
      "Epoch 3/125\n",
      "781/781 [==============================] - 42s 54ms/step - loss: 1.0628 - acc: 0.6495 - val_loss: 1.0312 - val_acc: 0.6796\n",
      "Epoch 4/125\n",
      "781/781 [==============================] - 41s 53ms/step - loss: 0.9808 - acc: 0.6846 - val_loss: 0.8204 - val_acc: 0.7452\n",
      "Epoch 5/125\n",
      "781/781 [==============================] - 41s 53ms/step - loss: 0.9211 - acc: 0.7090 - val_loss: 0.8699 - val_acc: 0.7419\n",
      "Epoch 6/125\n",
      "781/781 [==============================] - 41s 52ms/step - loss: 0.8817 - acc: 0.7271 - val_loss: 0.8400 - val_acc: 0.7521\n",
      "Epoch 7/125\n",
      "781/781 [==============================] - 43s 55ms/step - loss: 0.8527 - acc: 0.7420 - val_loss: 0.8409 - val_acc: 0.7609\n",
      "Epoch 8/125\n",
      "781/781 [==============================] - 42s 54ms/step - loss: 0.8239 - acc: 0.7542 - val_loss: 0.8195 - val_acc: 0.7659\n",
      "Epoch 9/125\n",
      "781/781 [==============================] - 41s 53ms/step - loss: 0.8086 - acc: 0.7598 - val_loss: 0.7785 - val_acc: 0.7768\n",
      "Epoch 10/125\n",
      "781/781 [==============================] - 41s 53ms/step - loss: 0.8000 - acc: 0.7648 - val_loss: 0.7578 - val_acc: 0.7834\n",
      "Epoch 11/125\n",
      "781/781 [==============================] - 41s 53ms/step - loss: 0.7774 - acc: 0.7734 - val_loss: 0.7301 - val_acc: 0.7944\n",
      "Epoch 12/125\n",
      "781/781 [==============================] - 43s 55ms/step - loss: 0.7669 - acc: 0.7789 - val_loss: 0.6909 - val_acc: 0.8135\n",
      "Epoch 13/125\n",
      "781/781 [==============================] - 43s 54ms/step - loss: 0.7495 - acc: 0.7862 - val_loss: 0.7026 - val_acc: 0.8085\n",
      "Epoch 14/125\n",
      "781/781 [==============================] - 41s 53ms/step - loss: 0.7442 - acc: 0.7902 - val_loss: 0.7646 - val_acc: 0.7905\n",
      "Epoch 15/125\n",
      "781/781 [==============================] - 40s 51ms/step - loss: 0.7345 - acc: 0.7919 - val_loss: 0.7020 - val_acc: 0.8058\n",
      "Epoch 16/125\n",
      "781/781 [==============================] - 42s 54ms/step - loss: 0.7298 - acc: 0.7966 - val_loss: 0.6917 - val_acc: 0.8123\n",
      "Epoch 17/125\n",
      "781/781 [==============================] - 40s 51ms/step - loss: 0.7179 - acc: 0.8002 - val_loss: 0.7052 - val_acc: 0.8145\n",
      "Epoch 18/125\n",
      "781/781 [==============================] - 39s 49ms/step - loss: 0.7163 - acc: 0.8027 - val_loss: 0.6582 - val_acc: 0.8296\n",
      "Epoch 19/125\n",
      "781/781 [==============================] - 39s 50ms/step - loss: 0.7062 - acc: 0.8048 - val_loss: 0.6746 - val_acc: 0.8235\n",
      "Epoch 20/125\n",
      "781/781 [==============================] - 42s 54ms/step - loss: 0.7043 - acc: 0.8070 - val_loss: 0.6503 - val_acc: 0.8279\n",
      "Epoch 21/125\n",
      "781/781 [==============================] - 41s 53ms/step - loss: 0.6966 - acc: 0.8120 - val_loss: 0.6444 - val_acc: 0.8325\n",
      "Epoch 22/125\n",
      "781/781 [==============================] - 40s 51ms/step - loss: 0.6903 - acc: 0.8123 - val_loss: 0.6576 - val_acc: 0.8317\n",
      "Epoch 23/125\n",
      "781/781 [==============================] - 39s 50ms/step - loss: 0.6848 - acc: 0.8158 - val_loss: 0.6515 - val_acc: 0.8325\n",
      "Epoch 24/125\n",
      "781/781 [==============================] - 43s 54ms/step - loss: 0.6818 - acc: 0.8187 - val_loss: 0.6676 - val_acc: 0.8310\n",
      "Epoch 25/125\n",
      "781/781 [==============================] - 42s 54ms/step - loss: 0.6831 - acc: 0.8163 - val_loss: 0.6403 - val_acc: 0.8382\n",
      "Epoch 26/125\n",
      "781/781 [==============================] - 41s 52ms/step - loss: 0.6771 - acc: 0.8208 - val_loss: 0.6234 - val_acc: 0.8392\n",
      "Epoch 27/125\n",
      "781/781 [==============================] - 40s 51ms/step - loss: 0.6690 - acc: 0.8212 - val_loss: 0.7271 - val_acc: 0.8139\n",
      "Epoch 28/125\n",
      "781/781 [==============================] - 39s 50ms/step - loss: 0.6707 - acc: 0.8225 - val_loss: 0.6347 - val_acc: 0.8384\n",
      "Epoch 29/125\n",
      "781/781 [==============================] - 43s 54ms/step - loss: 0.6639 - acc: 0.8248 - val_loss: 0.6386 - val_acc: 0.8429\n",
      "Epoch 30/125\n",
      "781/781 [==============================] - 42s 54ms/step - loss: 0.6595 - acc: 0.8267 - val_loss: 0.6124 - val_acc: 0.8503\n",
      "Epoch 31/125\n",
      "781/781 [==============================] - 41s 53ms/step - loss: 0.6635 - acc: 0.8281 - val_loss: 0.6454 - val_acc: 0.8368\n",
      "Epoch 32/125\n",
      "781/781 [==============================] - 40s 52ms/step - loss: 0.6623 - acc: 0.8257 - val_loss: 0.6470 - val_acc: 0.8402\n",
      "Epoch 33/125\n",
      "781/781 [==============================] - 42s 54ms/step - loss: 0.6559 - acc: 0.8276 - val_loss: 0.6664 - val_acc: 0.8342\n",
      "Epoch 34/125\n",
      "781/781 [==============================] - 41s 53ms/step - loss: 0.6483 - acc: 0.8328 - val_loss: 0.6192 - val_acc: 0.8478\n",
      "Epoch 35/125\n",
      "781/781 [==============================] - 40s 51ms/step - loss: 0.6486 - acc: 0.8319 - val_loss: 0.6265 - val_acc: 0.8458\n",
      "Epoch 36/125\n",
      "781/781 [==============================] - 39s 50ms/step - loss: 0.6491 - acc: 0.8326 - val_loss: 0.6558 - val_acc: 0.8396\n",
      "Epoch 37/125\n",
      "781/781 [==============================] - 43s 55ms/step - loss: 0.6435 - acc: 0.8347 - val_loss: 0.6038 - val_acc: 0.8542\n",
      "Epoch 38/125\n",
      "781/781 [==============================] - 43s 54ms/step - loss: 0.6472 - acc: 0.8326 - val_loss: 0.6185 - val_acc: 0.8508\n",
      "Epoch 39/125\n",
      "781/781 [==============================] - 42s 54ms/step - loss: 0.6407 - acc: 0.8356 - val_loss: 0.6245 - val_acc: 0.8472\n",
      "Epoch 40/125\n",
      "781/781 [==============================] - 42s 53ms/step - loss: 0.6421 - acc: 0.8362 - val_loss: 0.6358 - val_acc: 0.8426\n",
      "Epoch 41/125\n",
      "781/781 [==============================] - 41s 52ms/step - loss: 0.6344 - acc: 0.8378 - val_loss: 0.6333 - val_acc: 0.8451\n",
      "Epoch 42/125\n",
      "781/781 [==============================] - 41s 52ms/step - loss: 0.6365 - acc: 0.8378 - val_loss: 0.6178 - val_acc: 0.8539\n",
      "Epoch 43/125\n",
      "781/781 [==============================] - 43s 55ms/step - loss: 0.6358 - acc: 0.8367 - val_loss: 0.6828 - val_acc: 0.8309\n",
      "Epoch 44/125\n",
      "781/781 [==============================] - 43s 55ms/step - loss: 0.6304 - acc: 0.8407 - val_loss: 0.6373 - val_acc: 0.8428\n",
      "Epoch 45/125\n",
      "781/781 [==============================] - 42s 54ms/step - loss: 0.6347 - acc: 0.8390 - val_loss: 0.6374 - val_acc: 0.8440\n",
      "Epoch 46/125\n",
      "781/781 [==============================] - 41s 53ms/step - loss: 0.6296 - acc: 0.8407 - val_loss: 0.6277 - val_acc: 0.8487\n",
      "Epoch 47/125\n",
      "781/781 [==============================] - 41s 52ms/step - loss: 0.6277 - acc: 0.8404 - val_loss: 0.6248 - val_acc: 0.8527\n",
      "Epoch 48/125\n",
      "781/781 [==============================] - 44s 56ms/step - loss: 0.6247 - acc: 0.8427 - val_loss: 0.6162 - val_acc: 0.8513\n",
      "Epoch 49/125\n",
      "781/781 [==============================] - 43s 55ms/step - loss: 0.6300 - acc: 0.8406 - val_loss: 0.6903 - val_acc: 0.8292\n",
      "Epoch 50/125\n",
      "781/781 [==============================] - 42s 54ms/step - loss: 0.6225 - acc: 0.8433 - val_loss: 0.6257 - val_acc: 0.8573\n",
      "Epoch 51/125\n",
      "781/781 [==============================] - 41s 53ms/step - loss: 0.6252 - acc: 0.8427 - val_loss: 0.6078 - val_acc: 0.8566\n",
      "Epoch 52/125\n",
      "781/781 [==============================] - 41s 52ms/step - loss: 0.6192 - acc: 0.8443 - val_loss: 0.6148 - val_acc: 0.8538\n",
      "Epoch 53/125\n",
      "781/781 [==============================] - 43s 55ms/step - loss: 0.6178 - acc: 0.8454 - val_loss: 0.6165 - val_acc: 0.8486\n",
      "Epoch 54/125\n",
      "781/781 [==============================] - 41s 53ms/step - loss: 0.6207 - acc: 0.8454 - val_loss: 0.5953 - val_acc: 0.8564\n",
      "Epoch 55/125\n",
      "781/781 [==============================] - 40s 51ms/step - loss: 0.6213 - acc: 0.8448 - val_loss: 0.6065 - val_acc: 0.8591\n",
      "Epoch 56/125\n",
      "781/781 [==============================] - 39s 50ms/step - loss: 0.6185 - acc: 0.8468 - val_loss: 0.6013 - val_acc: 0.8532\n",
      "Epoch 57/125\n",
      "781/781 [==============================] - 42s 54ms/step - loss: 0.6139 - acc: 0.8468 - val_loss: 0.6062 - val_acc: 0.8561\n",
      "Epoch 58/125\n",
      "781/781 [==============================] - 40s 51ms/step - loss: 0.6164 - acc: 0.8477 - val_loss: 0.6568 - val_acc: 0.8497\n",
      "Epoch 59/125\n",
      "781/781 [==============================] - 41s 53ms/step - loss: 0.6126 - acc: 0.8468 - val_loss: 0.5679 - val_acc: 0.8691\n",
      "Epoch 60/125\n",
      "781/781 [==============================] - 40s 51ms/step - loss: 0.6106 - acc: 0.8486 - val_loss: 0.6254 - val_acc: 0.8551\n",
      "Epoch 61/125\n",
      "781/781 [==============================] - 40s 52ms/step - loss: 0.6132 - acc: 0.8489 - val_loss: 0.6176 - val_acc: 0.8563\n",
      "Epoch 62/125\n",
      "781/781 [==============================] - 43s 55ms/step - loss: 0.6175 - acc: 0.8453 - val_loss: 0.6307 - val_acc: 0.8507\n",
      "Epoch 63/125\n",
      "781/781 [==============================] - 41s 52ms/step - loss: 0.6113 - acc: 0.8476 - val_loss: 0.5975 - val_acc: 0.8574\n",
      "Epoch 64/125\n",
      "781/781 [==============================] - 41s 52ms/step - loss: 0.6110 - acc: 0.8489 - val_loss: 0.5769 - val_acc: 0.8656\n",
      "Epoch 65/125\n",
      "781/781 [==============================] - 44s 56ms/step - loss: 0.6091 - acc: 0.8479 - val_loss: 0.6175 - val_acc: 0.8503\n",
      "Epoch 66/125\n",
      "781/781 [==============================] - 42s 54ms/step - loss: 0.6055 - acc: 0.8503 - val_loss: 0.5838 - val_acc: 0.8673\n",
      "Epoch 67/125\n",
      "781/781 [==============================] - 41s 52ms/step - loss: 0.6031 - acc: 0.8508 - val_loss: 0.5799 - val_acc: 0.8640\n",
      "Epoch 68/125\n",
      "781/781 [==============================] - 40s 51ms/step - loss: 0.6107 - acc: 0.8499 - val_loss: 0.5603 - val_acc: 0.8677\n",
      "Epoch 69/125\n",
      "781/781 [==============================] - 39s 50ms/step - loss: 0.6068 - acc: 0.8496 - val_loss: 0.6240 - val_acc: 0.8559\n",
      "Epoch 70/125\n",
      "781/781 [==============================] - 41s 53ms/step - loss: 0.5965 - acc: 0.8532 - val_loss: 0.6044 - val_acc: 0.8582\n",
      "Epoch 71/125\n",
      "781/781 [==============================] - 40s 51ms/step - loss: 0.6041 - acc: 0.8496 - val_loss: 0.5856 - val_acc: 0.8671\n",
      "Epoch 72/125\n",
      "781/781 [==============================] - 39s 50ms/step - loss: 0.6049 - acc: 0.8518 - val_loss: 0.5980 - val_acc: 0.8614\n",
      "Epoch 73/125\n",
      "781/781 [==============================] - 40s 51ms/step - loss: 0.6038 - acc: 0.8520 - val_loss: 0.6096 - val_acc: 0.8567\n",
      "Epoch 74/125\n",
      "781/781 [==============================] - 38s 49ms/step - loss: 0.6016 - acc: 0.8522 - val_loss: 0.5725 - val_acc: 0.8667\n",
      "Epoch 75/125\n",
      "781/781 [==============================] - 40s 51ms/step - loss: 0.6032 - acc: 0.8526 - val_loss: 0.5663 - val_acc: 0.8682\n",
      "Epoch 76/125\n",
      "781/781 [==============================] - 40s 51ms/step - loss: 0.5972 - acc: 0.8535 - val_loss: 0.6739 - val_acc: 0.8389\n",
      "Epoch 77/125\n",
      "781/781 [==============================] - 40s 51ms/step - loss: 0.5529 - acc: 0.8674 - val_loss: 0.5306 - val_acc: 0.8825\n",
      "Epoch 78/125\n",
      "781/781 [==============================] - 40s 52ms/step - loss: 0.5350 - acc: 0.8731 - val_loss: 0.5364 - val_acc: 0.8814\n",
      "Epoch 79/125\n",
      "781/781 [==============================] - 43s 55ms/step - loss: 0.5277 - acc: 0.8752 - val_loss: 0.5404 - val_acc: 0.8765\n",
      "Epoch 80/125\n",
      "781/781 [==============================] - 42s 54ms/step - loss: 0.5178 - acc: 0.8770 - val_loss: 0.5362 - val_acc: 0.8760\n",
      "Epoch 81/125\n",
      "781/781 [==============================] - 41s 52ms/step - loss: 0.5170 - acc: 0.8770 - val_loss: 0.5292 - val_acc: 0.8785\n",
      "Epoch 82/125\n",
      "781/781 [==============================] - 40s 52ms/step - loss: 0.5104 - acc: 0.8769 - val_loss: 0.5559 - val_acc: 0.8722\n",
      "Epoch 83/125\n",
      "781/781 [==============================] - 40s 52ms/step - loss: 0.5084 - acc: 0.8761 - val_loss: 0.5109 - val_acc: 0.8835\n",
      "Epoch 84/125\n",
      "781/781 [==============================] - 43s 55ms/step - loss: 0.5006 - acc: 0.8790 - val_loss: 0.5399 - val_acc: 0.8762\n",
      "Epoch 85/125\n",
      "781/781 [==============================] - 41s 53ms/step - loss: 0.5084 - acc: 0.8766 - val_loss: 0.5449 - val_acc: 0.8726\n",
      "Epoch 86/125\n",
      "781/781 [==============================] - 40s 52ms/step - loss: 0.4970 - acc: 0.8792 - val_loss: 0.5216 - val_acc: 0.8795\n",
      "Epoch 87/125\n",
      "781/781 [==============================] - 39s 51ms/step - loss: 0.4966 - acc: 0.8786 - val_loss: 0.5171 - val_acc: 0.8787\n",
      "Epoch 88/125\n",
      "781/781 [==============================] - 43s 55ms/step - loss: 0.4959 - acc: 0.8793 - val_loss: 0.5047 - val_acc: 0.8845\n",
      "Epoch 89/125\n",
      "781/781 [==============================] - 40s 51ms/step - loss: 0.4931 - acc: 0.8785 - val_loss: 0.5330 - val_acc: 0.8726\n",
      "Epoch 90/125\n",
      "781/781 [==============================] - 39s 50ms/step - loss: 0.4862 - acc: 0.8803 - val_loss: 0.5312 - val_acc: 0.8736\n",
      "Epoch 91/125\n",
      "781/781 [==============================] - 40s 51ms/step - loss: 0.4864 - acc: 0.8808 - val_loss: 0.4913 - val_acc: 0.8856\n",
      "Epoch 92/125\n",
      "781/781 [==============================] - 43s 55ms/step - loss: 0.4851 - acc: 0.8819 - val_loss: 0.5740 - val_acc: 0.8665\n",
      "Epoch 93/125\n",
      "781/781 [==============================] - 41s 53ms/step - loss: 0.4818 - acc: 0.8803 - val_loss: 0.5446 - val_acc: 0.8705\n",
      "Epoch 94/125\n",
      "781/781 [==============================] - 39s 50ms/step - loss: 0.4813 - acc: 0.8797 - val_loss: 0.4934 - val_acc: 0.8824\n",
      "Epoch 95/125\n",
      "781/781 [==============================] - 59s 76ms/step - loss: 0.4772 - acc: 0.8816 - val_loss: 0.5111 - val_acc: 0.8792\n",
      "Epoch 96/125\n",
      "781/781 [==============================] - 43s 55ms/step - loss: 0.4836 - acc: 0.8811 - val_loss: 0.5183 - val_acc: 0.8760\n",
      "Epoch 97/125\n",
      "781/781 [==============================] - 41s 52ms/step - loss: 0.4786 - acc: 0.8812 - val_loss: 0.5390 - val_acc: 0.8715\n",
      "Epoch 98/125\n",
      "235/781 [========>.....................] - ETA: 26s - loss: 0.4688 - acc: 0.8828Buffered data was truncated after reaching the output size limit."
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import cifar10\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    " \n",
    "def lr_schedule(epoch):\n",
    "    lrate = 0.001\n",
    "    if epoch > 75:\n",
    "        lrate = 0.0005\n",
    "    elif epoch > 100:\n",
    "        lrate = 0.0003       \n",
    "    return lrate\n",
    "\n",
    "#z-score\n",
    "mean = np.mean(X_train,axis=(0,1,2,3))\n",
    "std = np.std(X_train,axis=(0,1,2,3))\n",
    "X_train = (X_train-mean)/(std+1e-7)\n",
    "X_test = (X_test-mean)/(std+1e-7)\n",
    " \n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train,num_classes)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes)\n",
    " \n",
    "weight_decay = 1e-4\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=X_train.shape[1:]))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    " \n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    " \n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    " \n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    " \n",
    "model.summary()\n",
    " \n",
    "#data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    )\n",
    "datagen.fit(X_train)\n",
    " \n",
    "#training\n",
    "batch_size = 64\n",
    " \n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\\\n",
    "                    steps_per_epoch=X_train.shape[0] // batch_size,epochs=125,\\\n",
    "                    verbose=1,validation_data=(X_test,y_test),callbacks=[LearningRateScheduler(lr_schedule)])\n",
    "#save to disk\n",
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights('model.h5') \n",
    "  \n",
    "#testing\n",
    "scores = model.evaluate(X_test, y_test, batch_size=128, verbose=1)\n",
    "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3hNaykiFFchX"
   },
   "source": [
    "**HIGHEST ACCURACY = val_acc: 0.8856**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XkoLi9R8ngvG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Q7 CNNCifar Hw5.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
